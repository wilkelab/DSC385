{
  "hash": "f5112ad94d81c98bdb6721293e63c2fc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Claus O. Wilke\"\nformat: live-html\nengine: knitr\nwebr:\n  render-df: gt-interactive\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## Introduction\n\nIn this worksheet, we will discuss how to perform k-means clustering.\n\nFirst we need to load the required R packages. Please wait a moment until the live R session is fully set up and all packages are loaded.\n\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| warning: false\n#| edit: false\nlibrary(tidyverse)\nlibrary(broom)\n```\n:::\n\n\n\n\nNext we set up the data.\n\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n#| warning: false\nspirals <- read_csv(\"https://wilkelab.org/DSC385/datasets/spirals.csv\")\n```\n:::\n\n\n\n\nWe will be working with two datasets, `spirals` and `iris`. The dataset `spirals` contains made-up data in two dimensions that forms three intertwined spirals.\n\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\nspirals\n```\n:::\n\n\n\n\nThe dataset `iris` contains measurements on the leaves of flowers of three *Iris* species.\n\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\niris\n```\n:::\n\n\n\n\n**Hint:** Pay attention to the column names in the `iris` dataset. They are all capitalized (e.g., `Species`), and the first four use a point as a separator (e.g., `Sepal.Length`). It is easy to misspell them and then the R code doesn't work correctly.\n\n## Clustering the `iris` dataset\n\nWe perform k-means clustering in R with the function `kmeans()`. It takes two important arguments, the number of clusters we want to generate (`centers`) and the number of times we want to re-run the clustering algorithm with different random starting points (`nstart`). Similarly to a PCA, we need to remove all non-numeric data columns before we can run the analysis.\n\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\nkm_fit <- iris |> \n  select(where(is.numeric)) |>\n  kmeans(centers = 3, nstart = 10)\n\nkm_fit\n```\n:::\n\n\n\n\nThe output from the fitted object (`km_fit`) gives us various pieces of information in human-readable form, such as the cluster sizes, the cluster means, and the assignment of rows in the original data table to the different clusters (\"clustering vector\").\n\nAt the end of the output, you see a list of \"available components.\" These are various pieces of information about the clustering result that you can extract from the fitted object. For example, `km_fit$cluster` gives you information about which row in the original data table belongs to which cluster.\n\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\nkm_fit$cluster\n```\n:::\n\n\n\n\nSimilarly, `centers` will give you the positions of the cluster centers and `tot.withinss` will give you the total within sum-of-squares. Try this out. Also, see if you can figure out what the component `size` represents.\n\n\n\n\n::: {.cell exercise='kmfit-extraction'}\n```{webr}\n#| exercise: kmfit-extraction\nkm_fit$___\n```\n:::\n\n\n\n\n::: { .hint exercise=\"kmfit-extraction\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\nkm_fit$cluster\n___\n___\n```\n:::\n:::\n\n::: { .solution exercise=\"kmfit-extraction\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\nkm_fit$cluster      # assignment of original data rows to clusters\nkm_fit$tot.withinss # total within sum-of-squares\nkm_fit$size         # cluster sizes (number of observations in each cluster)\n```\n:::\n:::\n\nNext we move on to plotting the clustering output. The k-means algorithm is a stochastic algorithm that produces slightly different output each time it is run. This is particularly apparent when you set `nstart = 1`. In this case, you will get possibly quite different results for different random seeds. You can set the random seed via `set.seed()`.\n\nIn the example below, try various seeds, including 2356, 2357, 2358, 2359, and see what the results are.\n\n\n\n\n::: {.cell exercise='kmeans-random'}\n```{webr}\n#| exercise: kmeans-random\nset.seed(2356)\n\nkm_fit <- iris |> \n  select(where(is.numeric)) |>\n  kmeans(centers = 3, nstart = 1)\n\nkm_fit |>\n  augment(iris) |>\n  ggplot() +\n  aes(x = Sepal.Length, Sepal.Width) +\n  geom_point(aes(color = .cluster, shape = Species))\n```\n:::\n\n\n\n\nNow set `nstart = 10` and try the same random seeds once more.\n\n## Finding the appropriate number of clusters\n\nTo get a sense of the correct number of clusters for a given dataset, we can plot the total within sum-of-squares as a function of the cluster number and look for a bend (\"elbow\") in the curve. Remember, the total within sum-of-squares can be obtained from the fitted object via `km_fit$tot.withinss`.\n\nThe following code sets up a function `calc_withinss` that calculates the total within sum-of-squares for an arbitrary data set and number of clusters, and then applies it to the `spirals` dataset.\n\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n# function to calculate within sum squares\ncalc_withinss <- function(data, centers) {\n  km_fit <- select(data, where(is.numeric)) |>\n    kmeans(centers = centers, nstart = 10)\n  km_fit$tot.withinss\n}\n\ntibble(centers = 1:15) |>\n  mutate(\n    within_sum_squares = map_dbl(\n      centers, ~calc_withinss(spirals, .x)\n    )\n  ) \n```\n:::\n\n\n\n\nNow take this code and use it to make a plot of the total within sum-of-squares against cluster number.\n\n\n\n\n::: {.cell exercise='tot-within-exercise'}\n```{webr}\n#| exercise: tot-within-exercise\n\n```\n:::\n\n\n\n\n::: { .hint exercise=\"tot-within-exercise\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\ntibble(centers = 1:15) |>\n  mutate(\n    within_sum_squares = map_dbl(\n      centers, ~calc_withinss(spirals, .x)\n    )\n  ) |>\n  ggplot(aes(___)) +\n  ___\n```\n:::\n:::\n\n::: { .solution exercise=\"tot-within-exercise\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\ntibble(centers = 1:15) |>\n  mutate(\n    within_sum_squares = map_dbl(\n      centers, ~calc_withinss(spirals, .x)\n    )\n  ) |>\n  ggplot(aes(centers, within_sum_squares)) +\n  geom_point() +\n  geom_line() +\n  theme_bw()\n```\n:::\n:::\n\nThe plot suggests that the correct number of clusters should be around 3 or 4. Now cluster the `spirals` dataset with this number of clusters and then plot it colored by cluster id.\n\n\n\n\n::: {.cell exercise='spirals-kmeans'}\n```{webr}\n#| exercise: spirals-kmeans\n\n```\n:::\n\n\n\n\n::: { .hint exercise=\"spirals-kmeans\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\nkm_fit <- ___ |> \n  select(where(is.numeric)) |>\n  kmeans(centers = 3, nstart = 10)\n\nkm_fit |>\n  augment(___) |>\n  ___\n```\n:::\n:::\n\n::: { .hint exercise=\"spirals-kmeans\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\nkm_fit <- spirals |> \n  select(where(is.numeric)) |>\n  kmeans(centers = 3, nstart = 10)\n\nkm_fit |>\n  augment(spirals) |>\n  ggplot() +\n  aes(___) +\n  geom_point(aes(color = ___, shape = ___))\n```\n:::\n:::\n\n\n::: { .solution exercise=\"spirals-kmeans\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\nkm_fit <- spirals |> \n  select(where(is.numeric)) |>\n  kmeans(centers = 3, nstart = 10)\n\nkm_fit |>\n  augment(spirals) |>\n  ggplot() +\n  aes(x, y) +\n  geom_point(aes(color = .cluster, shape = group))\n```\n:::\n:::\n\nTry a few different cluster numbers to see how the algorithm behaves. Do you think k-means clustering works on this dataset?\n\n## Combining k-means and PCA\n\nIn practice, we often perform PCA first on a dataset and then cluster the transformed coordinates. Try this out on the `iris` dataset. Run a PCA, then cluster the PCA coordinates, and then plot the clusters in PCA space.\n\nAs a reminder, this is how we would do a PCA on this dataset:\n\n\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\npca_fit <- iris |> \n  select(where(is.numeric)) |> # retain only numeric columns\n  scale() |>                   # scale to zero mean and unit variance\n  prcomp() \n\n# combine iris data with PCA data (needed for plot)\niris_pca <- augment(pca_fit, iris)\n\n# extract PC coordinates from the fitted object\n# (needed for k-means) \naugment(pca_fit) |>\n  select(-.rownames)  # remove columns with row names; not needed\n```\n:::\n\n\n\n\nNow modify this example so you perform a k-means clustering analysis and then color by clusters rather than by species.\n\n\n\n\n::: {.cell exercise='iris-pca-kmeans'}\n```{webr}\n#| exercise: iris-pca-kmeans\n\n```\n:::\n\n\n\n\n::: { .hint exercise=\"iris-pca-kmeans\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npca_fit <- iris |> \n  select(where(is.numeric)) |> # retain only numeric columns\n  scale() |>                   # scale to zero mean and unit variance\n  prcomp()\n\n# combine iris data with PCA data (needed for plot)\niris_pca <- augment(pca_fit, iris)\n\n# perform k-means\nkm_fit <- augment(pca_fit) |>\n  select(-.rownames) |>\n  ___\n```\n:::\n:::\n\n::: { .hint exercise=\"iris-pca-kmeans\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npca_fit <- iris |> \n  select(where(is.numeric)) |> # retain only numeric columns\n  scale() |>                   # scale to zero mean and unit variance\n  prcomp()\n\n# combine iris data with PCA data (needed for plot)\niris_pca <- augment(pca_fit, iris)\n\n# perform k-means\nkm_fit <- augment(pca_fit) |>\n  select(-.rownames) |>\n  kmeans(centers = 3, nstart = 10)\n\nkm_fit |>\n  # combine with original data and the PCA coordinates\n  ___ |>\n  # plot\n  ___\n```\n:::\n:::\n\n::: { .hint exercise=\"iris-pca-kmeans\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npca_fit <- iris |> \n  select(where(is.numeric)) |> # retain only numeric columns\n  scale() |>                   # scale to zero mean and unit variance\n  prcomp()\n\n# combine iris data with PCA data (needed for plot)\niris_pca <- augment(pca_fit, iris)\n\n# perform k-means\nkm_fit <- augment(pca_fit) |>\n  select(-.rownames) |>\n  kmeans(centers = 3, nstart = 10)\n\nkm_fit |>\n  # combine with original data and the PCA coordinates\n  augment(iris_pca) |>\n  ggplot() +\n  ___\n```\n:::\n:::\n\n::: { .hint exercise=\"iris-pca-kmeans\" }\n::: { .callout-tip title=\"Hint\" collapse=\"false\"}\n```r\npca_fit <- iris |> \n  select(where(is.numeric)) |> # retain only numeric columns\n  scale() |>                   # scale to zero mean and unit variance\n  prcomp()\n\n# combine iris data with PCA data (needed for plot)\niris_pca <- augment(pca_fit, iris)\n\n# perform k-means\nkm_fit <- augment(pca_fit) |>\n  select(-.rownames) |>\n  kmeans(centers = 3, nstart = 10)\n\nkm_fit |>\n  # combine with original data and the PCA coordinates\n  augment(iris_pca) |>\n  ggplot() +\n  aes(x = .fittedPC1, .fittedPC2) +\n  geom_point(\n    aes(color = .cluster, shape = Species)\n  )\n```\n:::\n:::\n\n::: { .solution exercise=\"iris-pca-kmeans\" }\n::: { .callout-tip title=\"Solution\" collapse=\"false\"}\n```r\npca_fit <- iris |> \n  select(where(is.numeric)) |> # retain only numeric columns\n  scale() |>                   # scale to zero mean and unit variance\n  prcomp()\n\n# combine iris data with PCA data (needed for plot)\niris_pca <- augment(pca_fit, iris)\n\n# perform k-means\nkm_fit <- augment(pca_fit) |>\n  select(-.rownames) |>\n  kmeans(centers = 3, nstart = 10)\n\nkm_fit |>\n  # combine with original data and the PCA coordinates\n  augment(iris_pca) |>\n  ggplot() +\n  aes(x = .fittedPC1, .fittedPC2) +\n  geom_point(\n    aes(color = .cluster, shape = Species)\n  ) +\n  geom_point(\n    data = tidy(km_fit),\n    aes(fill = cluster),\n    shape = 21, color = \"black\", size = 4\n  ) +\n  guides(color = \"none\")\n```\n:::\n:::\n\nChange which components you plot on the x and the y axis to try to get a sense of how the clusters are located in the 4-dimensional PC space.\n\n\n\n",
    "supporting": [
      "clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}